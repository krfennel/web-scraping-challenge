{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the Dependencies.\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    # return Browser('chrome', **executable_path, headless=False)\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    return Browser('chrome', **executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def scrape():\n",
    " #   browser = init_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    browser = init_browser()\n",
    "    \n",
    "    # NASA Mars News\n",
    "    # \n",
    "    # Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text.     \n",
    "    url_mars_news = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "    browser.visit(url_mars_news)\n",
    "    time.sleep(2)\n",
    "    response = browser.html\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    # Assign the text to variables that you can reference later.\n",
    "    news_title = soup.find_all('div',class_=\"content_title\")[1].text\n",
    "    # Collecting the paragraph text.\n",
    "    news_p = soup.find('div',class_=\"article_teaser_body\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # JPL Mars Space Images - Featured Image    \n",
    "    # Visit the url for JPL Featured Space Image.\n",
    "    featured_image_url = 'https://d2pn8kiwq2w21t.cloudfront.net/images/jpegPIA24493.width-1024.jpg'\n",
    "    # Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign\n",
    "    # the url string to a variable called featured_image_url.\n",
    "    browser.visit(featured_image_url)\n",
    "    time.sleep(2)\n",
    "    featured_image_url = browser.find_by_tag('img')['src']\n",
    "    # img_results = soup.find_all('img',class_='BaseImage')\n",
    "    featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Mars Facts\n",
    "    # Visit the Mars Facts webpage and use Pandas to scrape the table containing facts about the planet \n",
    "    # including Diameter, Mass, etc. \n",
    "    url_facts = 'https://space-facts.com/mars/'\n",
    "    browser.visit(url_facts)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Use Pandas to convert the data to a HTML table string.\n",
    "    tables = pd.read_html(url_facts)\n",
    "    table = tables[0]\n",
    "    table.columns = ['Parameters','Values']\n",
    "    mars_table = table.set_index('Parameters',inplace=True)\n",
    "    mars_table = table.to_html()\n",
    "    #mars_table.replace('\\n', '')\n",
    "    table.to_html('mars_table.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mars Hemispheres\n",
    "# Visit the USGS Astrogeology site to obtain high resolution images for each of Mars hemispheres.\n",
    "url_hemi = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url_hemi)\n",
    "time.sleep(2)\n",
    "response_2 = browser.html\n",
    "soup = BeautifulSoup(response_2,'html.parser')\n",
    "results = soup.find_all('div',class_='item')\n",
    "# Save both the image url string for the full resolution hemisphere image, and the Hemisphere title \n",
    "# containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "img_url = []\n",
    "title = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # using splinter to navigate to the full resolution image\n",
    "parent_window = browser.driver.current_window_handle\n",
    "title.append(result.h3.text)\n",
    "browser.click_link_by_partial_text(result.h3.text)\n",
    "time.sleep(1)\n",
    "browser.click_link_by_partial_text('Sample')\n",
    "# Assigning window names parent window is main window and any new tab opened is child window.\n",
    "all_windows = browser.driver.window_handles\n",
    "time.sleep(1)\n",
    "child_window = [window for window in all_windows if window != parent_window][0]\n",
    "# Switching the splinter to operate on the child window which contains the link for the image.\n",
    "browser.driver.switch_to.window(child_window)\n",
    "# Collecting the image url.\n",
    "img_url.append(browser.find_by_tag('img')['src'])\n",
    "# Closing the child window\n",
    "browser.driver.close()\n",
    "# Switching the splinter back to operate the parent window.\n",
    "browser.driver.switch_to.window(parent_window)\n",
    "browser.back()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls = []\n",
    "# Appending the dictionary with the image url string and the hemisphere title to a list. \n",
    "# This list will contain one dictionary for each hemisphere.\n",
    "for i in range(0,4):\n",
    "    hemisphere_image_urls.append({\"title\":title[i],\"image_url\":img_url[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Python dictionary containing all of the scraped data.\n",
    "    mars_data = {\n",
    "        \"news_title\":news_title,\n",
    "        \"news_p\" : news_p,\n",
    "        \"featured_image_url\" : featured_image_url,\n",
    "        \"mars_table\" : mars_table,\n",
    "        \"hemisphere_image_urls\" : hemisphere_image_urls\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Close the browser after scraping\n",
    "    browser.quit()\n",
    "    # Return Results\n",
    "    return mars_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
